<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PicoGPT</title>
    <link rel="stylesheet" href="https://use.typekit.net/upp3wws.css">
    <style>
        /* Base styles */
        body {
            font-family: "courier-std", monospace;
            margin: 0;
            padding: 0;
            background-color: #fff;
            color: #111;
            line-height: 1.6;
            font-size: 16px;
        }

                /* Base container tweaks */
        .container {
            max-width: 42rem;
            margin: 0 auto;
            padding: 2rem 1rem;
            position: relative; /* For positioning wider elements */
        }

        /* Typography */
        h1, h2, h3, .serif {
            font-family: Georgia, serif;
            font-weight: normal;
            line-height: 1.3;
            color: #000;
        }

        h1 {
            font-size: 2.5rem;
            margin: 1.5rem 0 1rem;
        }

        h2 {
            font-size: 2rem;
            margin: 2.5rem 0 1rem;
        }

        h3 {
            font-size: 1.5rem;
            margin: 2rem 0 0.75rem;
        }

        p {
            margin: 0 0 1.5rem;
        }

        /* Header and Navigation */
        header {
            border-bottom: 1px solid #eee;
            margin-bottom: 3rem;
            padding: 1rem 0;
        }

        .nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-title {
            font-family: Georgia, serif;
            font-size: 1.25rem;
            color: #000;
            text-decoration: none;
        }

        .nav-links {
            display: flex;
            gap: 1.5rem;
        }

        .nav-links a {
            color: #111;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s;
        }

        .nav-links a:hover {
            border-bottom-color: #111;
        }

        /* Article Styling */
        .post-header {
            margin-bottom: 3rem;
        }

        .post-meta {
            font-size: 0.875rem;
            color: #666;
            margin: 0.5rem 0 2rem;
        }

        .post-content {
            margin-bottom: 4rem;
        }

        /* Code Blocks */
        pre {
            background-color: #f5f5f5;
            padding: 1rem;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        .code-block {
            position: relative;
            width: 60rem; /* Significantly wider than content */
            left: 50%;
            transform: translateX(-50%);
            margin: 1.5rem 0;
            font-size: 0.9em;
        }

        .code-preview {
            background-color: #f5f5f5;
            padding: 1rem;
            border-radius: 4px 4px 0 0;
            cursor: pointer;
            border: 1px solid #eee;
            white-space: pre;
        }

        .code-full {
            display: none;
            background-color: #f5f5f5;
            padding: 1rem;
            border-radius: 0 0 4px 4px;
            border: 1px solid #eee;
            border-top: none;
            white-space: pre;
        }

        .code-block.expanded .code-full {
            display: block;
        }

        .code-preview::after {
            content: "▼";
            position: absolute;
            right: 1rem;
            top: 1rem;
            color: #666;
        }

        .code-block.expanded .code-preview::after {
            content: "▲";
        }

        code {
            font-family: "courier-std", monospace;
            tab-size: 4;
            -moz-tab-size: 4;
            display: block;
        }

        /* Keep inline code styling */
        p code {
            background-color: #f5f5f5;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            white-space: normal;
            display: inline;
        }

        /* Responsive adjustments */
        @media (max-width: 65rem) {
            .code-block {
                width: calc(100vw - 2rem);
                font-size: 0.85em;
            }
        }

        /* Links */
        a {
            color: #111;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 0.2em;
        }

        a:hover {
            color: #444;
        }

        /* Back Link */
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: #666;
            text-decoration: none;
        }

        .back-link:hover {
            color: #111;
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid #eee;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            margin: 2rem 0;
            border-radius: 4px;
        }

        /* Responsive Design */
        @media (max-width: 600px) {
            body {
                font-size: 15px;
            }
            
            .container {
                padding: 1rem;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            h2 {
                font-size: 1.75rem;
            }
            
            h3 {
                font-size: 1.25rem;
            }
        }
    </style>


<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Convert existing code blocks to the new format
        document.querySelectorAll('pre').forEach(function(preElement) {
            const codeBlock = document.createElement('div');
            codeBlock.className = 'code-block';
            
            const codeContent = preElement.textContent;
            const firstLine = codeContent.split('\n')[0];
            
            const preview = document.createElement('div');
            preview.className = 'code-preview';
            preview.innerHTML = `<code>${firstLine.trim()}</code>`;
            
            const fullCode = document.createElement('div');
            fullCode.className = 'code-full';
            fullCode.innerHTML = `<code>${codeContent}</code>`;
            
            codeBlock.appendChild(preview);
            codeBlock.appendChild(fullCode);
            
            codeBlock.addEventListener('click', function() {
                this.classList.toggle('expanded');
            });
            
            preElement.replaceWith(codeBlock);
        });
    });
</script>
</head>
<body>
    <div class="container">
        <header>
            <nav class="nav">
                <a href="/" class="nav-title">rishi</a>
                <div class="nav-links">
                    <a href="/writings">Writings</a>
                </div>
            </nav>
        </header>

        <main>
            <article>
                <a href="/writings" class="back-link">← Back to writings</a>
                
                <header class="post-header">
                    <h1>PicoGPT (Part 1/n)</h1>
                    <div class="post-meta">
                        *10 min read
                    </div>
                </header>
                
                <div class="post-content">
                    <h2>Overview</h2>
                    <p>
                        I made PicoGPT as a way to do small experiments on gpt2 small sized architectures. Primarily this comes from experience; even top labs tend to do a lot of 1 8xH100 node experiments before scaling.
                    It is loosely based on the NanoGPT speedrun repository, but I removed some features I think would not scale well, as the purpose of PicoGPT was not speedrunning, but instead doing experiments to form theories on how to make better models at larger scales. 
                    This is not to say that certain architectural changes NanoGPT made won’t scale, but moreso that I didn’t have evidence of scaling, so I decided to keep the experimental repository simple. 
                    Also, if I were speedrunning, then that severely limits the types of experiments in the design space (for example, MoEs are out of the question for speedrunning due to communication overhead (2 gathers, scatter)). <br>
                    The initial PicoGPT experiments include: <br>
                    <br>

                    Multi-latent Attention <br>
                    Flex Attention <br>
                    Relu**2 <br>
                    RoPE <br>
                    feed forward MoEs with expert parallelism (experiment 2) <br>

                    I'll probably do many more experiments on this repo, the design space is huge. I often think about: [SSMs, hybrid models, parameter sharing], which readily fit into model experiments at this size. <br>
                    Things I did not include from NanoGPT:<br>
                    <br>
                    tanh logit soft capping (gemma)<br>
                    long short sliding window attention (gemma)<br>
                    QK norm<br>
                    encoder/decoder u-net architecture (nanoGPT)<br>
                    Muon optimizer<br>
                    Window warmup (it was found by NanoGPT that slowly increasing context length during training to max sped up training, intuitively I buy this, but didn’t include it)<br>
                    The initial architecture is slightly more sample efficient than gpt2 on 5B tokens of fineweb (did not do a full fineweb10B token run). It has a lot of room for growth of course.
                    </p>
                    
                    <h2>Initial Hypothesis</h2>
                    <p>
                        My initial idea was as follows. Multi latent attention is really interesting because of the parameter savings from low rank projections, as well as the inference compute savings of caching a low rank RoPE key tensor and a low rank joint kv embedding. I wanted to run some experiments to loosely benchmark the capabilities of this architectural change and see if I could potentially improve upon it at a smaller scale. <br>
                    Pretty much every other company to my knowledge was using context parallel instead of being able to run only data parallel for attention computation. Deepseek was already paying the cost of having many MoE’s with expert parallel. Potentially due to the low-rank nature of the key and value projections, it might be worthwhile to add MoE’s for added expressivity here.
                    
                    
                    </p>
                    <h2>Background/Relevant Papers</h2>
                    <p>
                        There is the obvious work in Deepseek-V3 and Deepseek-V2 which you should read immediately. In those works they describe MLA, it’s really not a complicated architectural intervention, but it works. It’s probably important to read some of the other work with low rank attention such as Grouped Query Attention. In GQA they do a similar low rank projection such that they share key and value heads for multiple query heads. This is not as performant as MLA which suggests some interesting theories with regards to how attention works. <br>
                        I initially got the idea to add MoEs to the attention block from reading the MoEUT paper, as well as the Switch-Head paper, since they got somewhat good results from adding MoEs to value and output projections. Their paper found that messing with the inner product by adding MoEs to query and key projections is problematic.  
                        
                    </p>
                    <h2>Multi Latent Attention Implementation</h2>
                    <p>
                        Of course, from Deepseek-v2/v3 I had to dramatically scale down the MLA implementation. There is a large design space here as well, and I highly doubt I found the optimal configuration given my compute budget. 

                    </p>
                    <!-- Example code block -->
                    <pre><code>
                    class MultiLatentAttention(nn.Module):
                        def __init__(self,hidden_dim,num_heads=12,low_rank=2,block_size=128,max_seq_len=1024):
                            super().__init__()
                            self.num_heads=num_heads
                            self.head_dim=hidden_dim//num_heads
                            self.block_size=block_size
                            self.max_seq_len=max_seq_len
                            #assert hidden_dim//num_heads
                            #downproj for q
                            self.qd_proj=nn.Linear(hidden_dim,hidden_dim//low_rank) 
                            self.qu_proj=nn.Linear(hidden_dim//low_rank,hidden_dim)
                            
                            self.qr_proj=nn.Linear(hidden_dim,self.head_dim)
                            #shared downproj for k,v
                            self.kvd=nn.Linear(hidden_dim,hidden_dim//low_rank)
                            self.v_up_proj=nn.Linear(hidden_dim//low_rank,hidden_dim*2)
                            self.k_up_proj=nn.Linear(hidden_dim//low_rank,hidden_dim)
                            
                            self.kr_proj=nn.Linear(hidden_dim,self.head_dim)
                            #output proj
                            self.o_proj = nn.Linear(hidden_dim*2, hidden_dim)
                            
                            self.rope=RotaryPositionEmbedding(self.head_dim)
                            self.scale = (2*self.head_dim) ** -0.5 
                            
                            self.world_size=torch.distributed.get_world_size()
                            
                        
                    
                        def forward(self, x,token_seq):
                            #layer norm prior to input
                            B, N,dim = x.shape
                            assert B == 1, "Must use batch size = 1 for FlexAttention"
                            
                            
                            # query projections
                            qd=self.qd_proj(x) #B,N,low_rank_dim
                            qr=self.qr_proj(x).unsqueeze(2)# B,N,1,head_dim/2
                            qr=qr.expand(-1,-1,self.num_heads,-1).permute(0,2,1,3) #B,num_heads,seq_len,head_dim//2
                            qr=self.rope(qr)
                            q=self.qu_proj(qd) #B,N,dim
                            q=q.reshape(B,N,self.num_heads,self.head_dim).permute(0,2,1,3)
                            q=torch.cat((q,qr),dim=-1) #B,num_heads,seq_len,head_dim
                            
                            
                            #keys
                            low_rank_kv=self.kvd(x) #B,S,compressed_dim
                            k=self.k_up_proj(low_rank_kv)
                            kr=self.kr_proj(x).unsqueeze(2)
                            kr=kr.expand(-1,-1,self.num_heads,-1).permute(0,2,1,3)
                            kr=self.rope(kr)
                            k= k.reshape(B,N,self.num_heads,self.head_dim).permute(0,2,1,3)
                            k=torch.cat((k,kr),dim=-1) #B,num_heads,seq_len,head_dim
                            
                            #values
                            ### the point of doing low rank is not just parameter count reduction, but also kv cache size reduction
                            v=self.v_up_proj(low_rank_kv) 
                            v=v.reshape(B,N,self.num_heads,(self.head_dim*2)).permute(0,2,1,3)
                    
                            
                            docs = (token_seq == 50256).cumsum(0)
                            def document_causal_mask(b, h, q_idx, kv_idx):
                              causal_mask = q_idx >= kv_idx
                              document_mask = docs[q_idx] == docs[kv_idx]
                              window_mask = q_idx - kv_idx < 1024
                              return causal_mask & document_mask & window_mask
                    
                            S = len(token_seq)
                            block_mask = create_block_mask(document_causal_mask, None, None, S, S, device="cuda", _compile=True)
                    
                            x = flex_attention(q, k, v, block_mask=block_mask, scale=self.scale)
                            x = x.transpose(1, 2).reshape(B, N, -1)
                    
                            x=self.o_proj(x)
                            return x
</code></pre>
                </div>
            </article>
        </main>

        <footer>
            <p>© 2025 rishi</p>
        </footer>
    </div>
</body>
</html>